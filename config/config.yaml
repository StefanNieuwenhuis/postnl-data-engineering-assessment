# Gold layer output paths
gold:
  route_performance: "route_performance"

# Per-layer buckets: landing, bronze, silver, gold (paths are bucket root).
storage:
  local:
    buckets:
      landing: "s3a://landing"
      bronze: "s3a://bronze"
      silver: "s3a://silver"
      gold: "s3a://gold"
  databricks:
    buckets:

# Per-dataset: path within each layer bucket.
# source = path in landing (file or directory). When stream: true, source must be a directory.
# stream: true = Spark Structured Streaming (file stream).
datasets:
  routes:
    source: "sources/routes"
    format: "json"
    stream: true
    bronze_table: "raw_routes"
    silver_table: "clean_routes"
  shipments:
    source: "sources/shipments.csv"
    format: "csv"
    bronze_table: "raw_shipments"
    silver_table: "clean_shipments"
  vehicles:
    source: "sources/vehicles.csv"
    format: "csv"
    bronze_table: "raw_vehicles"
    silver_table: "clean_vehicles"
  weather:
    source: "sources/weather"
    format: "json"
    stream: true
    bronze_table: "raw_weather"
    silver_table: "clean_weather"

# Merge keys for idempotency: Delta MERGE uses these to upsert (no duplicates on re-run).
# Bronze: include source_file for batch (dedupe by file+key); stream uses business key only.
# Silver: business keys only (same as dedupe logic but for MERGE).
merge_keys:
  shipments:
    - "shipment_id"
  vehicles:
    - "vehicle_id"
  routes:
    - "route_id"
  weather:
    - "date"
    - "region"

transformations:
  silver:
    shipments:
      date_columns:
        - "ship_date"
      datetime_columns:
        - "planned_arrival"
        - "actual_arrival"
      dedupe_keys:
        - "shipment_id"
      required_columns:
        - "shipment_id"
        - "route_id"
        - "vehicle_id"
        - "carrier_id"
    routes:
      dedupe_keys:
        - "route_id"
    vehicles:
      dedupe_keys:
        - "vehicle_id"
    weather:
      dedupe_keys:
        - "date"
        - "region"


# Spark configuration
spark:
  local:
    app_name: "RoutePerformancePipeline"
    master: "local[*]"
    config:
      spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
      spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
      spark.hadoop.fs.s3a.endpoint: "http://localhost:9000"
      spark.hadoop.fs.s3a.access.key: "minio"
      spark.hadoop.fs.s3a.secret.key: "minio123"
      spark.hadoop.fs.s3a.path.style.access: "true"
      spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
      spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
      spark.hadoop.fs.s3a.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
      spark.hadoop.fs.s3a.threads.max: 64
      spark.hadoop.fs.s3a.connection.maximum: 64
      spark.databricks.delta.retentionDurationCheck.enabled: "false"
      spark.sql.streaming.schemaInference: "true"
      spark.sql.adaptive.enabled: "true"
